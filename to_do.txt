manually inspect all positive data samples in plot view with peak detection

compare with a robust variety of other samples

#clean up initial peak indexes in peak detection


revisit the peak cancelors 
- how are high amp peaks chosen.... manual inspection may yield a hard limit to use on DWT sigs vice a value on baseline which could be insignificant on good sigs
- revist peak width calcs
- revisit / document false peaks (mirrors?) in code

pulse train cancellation may not be correct


















rerun feature extraction for train (and test why not?)

rerun analysis with varying train test split monte carlos with new extracted features to see resilience against cherry picking

goal is consistent MCC > 0.5 for next submission

work in VSB_Final Notebook and elsewhere?





cross validation

over fitting


light gbm good for unbalanced
some parameter tweaks but caps out / don't know what i'm doing

revisit RF and try to tailor it for unbalanced
	- ensemble CV
	- set class importance
	- over predict

A. Use Ensemble Cross-Validation (CV): In this project, I used cross-validation to justify the model robustness. The entire datasets were divided into five subsets. In each CV, 4 out of 5 subsets are used for training, and remaining set was used to validate the model. In each CV, the model also predicts (probabilities, not the class) the test data. At the end of the cross-validation, we have five testing prediction probabilities. Finally, I average the prediction probabilities for all class. Training performance of the model was steady and has the almost constant recall and f1 score on each CV. This technique helped me predicting test data very well in one of the Kaggle competitions in which I became top 25th out of 5355 which is top 1%. The following partial code snippets shows the implementation of the Ensemble cross-validation:

B. Set Class Weight/Importance: Cost-sensitive learning is among the many other approaches to make the random forest more suitable for learning from very imbalanced data. The RF has the tendency to be biased on the majority class. Therefore, imposing a costly penalty on the minority class misclassification can be useful. Since this technique is proven the way of improving model performance, I assign a high weight to the minority class (i.e., higher misclassification cost). The class weights are then incorporated into the RF algorithm. I determine a class weight from the ratio between the number of the dataset in class-1 and the number of the dataset in the class. For example, the ratio between the number of datasets in class-1 and class-3 is approximately 110, and the ratio for class-1 and class-2 is about 26. Later, I slightly modify the number for improving the model performance in trail and error basis. The following code snippets show the implementation of the different class weights.

C. Over-Predict a Label than Under-Predict: This is technique is optional. I have applied this technique since I was asked to implement. It looks to me this method is very affecting to improve minority class performance. In brief, the technique is to penalize the model most if it misclassified class-3, a little less for class-2 and the least for class-1.

To implement the method, I changed the probability threshold for each class. To do so, I set the probability for class-3, class-2 and class-1 in increasing order (i.e, class-3 = 0.25, class-2 = 0.35, class-1 = 0.50), so that the model is forced to overpredict class. Detail implementation of this algorithm can be found on this project Github page.




work done has not tailored weights yet or done anything to fix / tailor the re-predict
